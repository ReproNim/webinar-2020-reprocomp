<!doctype html>
<html lang="en">
<head>
  <title>Reproducible Execution of Data Collection/Processing </title>
  <meta name="description" content="Slides for the ReproNim Webinar
									talk which walks through various
									aspects of a study to make it more
									reproducible ">
  <meta name="author" content=" Yaroslav O. Halchenko ">

  <meta charset="utf-8">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="css/main.css" id="theme">
  <script src="js/printpdf.js"></script>
</head>
<body>

<div class="reveal">
<div class="slides">


<section>
<section>

  <a href="http://centerforopenneuroscience.org/"><img data-src="pics/con-ccn-dartmouth-letterhead.svg"></a>

  <h3>Reproducible Execution of Data Collection/Processing</h3>

  <!--
  <table style="border:none">
    <tr>
      <td>
		<img style="height:150px;margin-bottom:30px" data-src="pics/datalad_logo_wide.svg">
	  </td>
        <td>
            <h3>Use of MRI QC metrics, scanning parameters 
and seasonal variation data to control for variance in MRI studies</h3>
        </td>
    </tr>
</table> -->

  <div style="margin-top:0em;text-align:center">
  <table style="border: none;">
  <tr>
	<td>Yaroslav O. Halchenko
	  <br><small>
		<a href="https://twitter.com/yarikoptic" target="_blank">
		  <img data-src="pics/twitter.png" style="height:30px;margin:0px" />
		  @yarikoptic</a></small></td>
  </tr>
  <tr>
    <td>
        <small><a href="http://centerforopenneuroscience.org/" target="_blank">Center for Open Neuroscience</a>
          <br><a href="https://pbs.dartmouth.edu/" target="_blank">Department of Psychological and Brain Sciences</a>
          <br><a href="https://www.dartmouth.edu/ccn/" target="_blank">Center for Cognitive Neuroscience</a><br>
		  <a href="http://www.dartmouth.edu" target="_blank">Dartmouth College</a></small>
		<!--<img style="height:150px;" data-src="pics/con-logo_blue_big.svg">-->
    </td>
  </tr>
  </table>
  </div>
  <br>
    <small>
	 <a href="http://repronim.org" target="_blank"> <img  style="height:150px;margin:20px"  data-src="pics/repronim-logo-vertical.svg"/></a>
	 <a href="http://datalad.org" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/datalad_D.svg"/></a>
	 <a href="http://neuro.debian.net" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/neurodebian.png"/></a>
	 <a href="https://bids.neuroimaging.io" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/BIDS_Logo.png"/></a>
	 <a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf" target="_blank"> <img  style="height:150px;margin:20px" data-src="pics/yoda.svg"/></a>
</small>
</section>

<!--
<section>
  <h2>Acknowledgements</h2>
  <table>
  <tr style="vertical-align:middle">
    <td style="vertical-align:middle">
      <dl>
        <dt style="margin-top:20px">QA data & DBIC support</dt>
        <dd style="margin-left:5px!important">
          <ul style="margin-left:5px!important">
            <li>Chandana Kodiweera</li>
			<li>Terry Sackett</li>
          </ul>
        </dd>
        <dt style="margin-top:20px">MRI data</dt>
        <dd style="margin-left:5px!important">
          <ul style="margin-left:5px!important">
			<li>Ida Gobbini</li>
			<li>James Haxby</li>
            <li>Luke Chang</li>
			<li>Eshin Jolly</li>
          </ul>
        </dd>
      </dl>
    </td>
	<td style="vertical-align:middle">
	  <div style="margin-left:40px;margin-bottom:-20px;text-align:center"><strong>Funders</strong></div>
	  <div style="margin-left:40px;text-align: left">
		<br>
		<img style="height:150px;margin-right:50px" data-src="pics/nsf1.jpg" />
		<ul style="margin-left:5px!important">
		  <li>1429999</li>
		  <li>1912266</li>
		  <li>1835200<br></li>
		</ul>
		<br><br><img style="height:150px;margin-right:50px" data-src="pics/nih.png" />
          <ul style="margin-left:5px!important">
            <li>P41EB019936</li>
			<li>R01MH116026</li>
			<li>R56MH080716</li>
			<li>R01MH116026</li>
		  </ul>
		  </div>
	</td>
  </tr>
</div>
<!- -
  <div style="margin-top:40px;margin-bottom:20px;text-align:center"><strong>Collaborators</strong></div>
  <div style="margin-top:-20px">
  <img style="height:100px;margin:20px" data-src="pics/hbp_logo.png" />
  <img style="height:100px;margin:20px" data-src="pics/conp_logo.png" />
  <img style="height:100px;margin:20px" data-src="pics/vbc_logo.png" />
  </div>
  <div style="margin-top:-40px">
  <img style="height:120px;margin:20px" data-src="pics/openneuro_logo.png" />
  <img style="height:120px;margin:20px" data-src="pics/cbrain_logo.png" />
  <img style="height:140px;margin:20px" data-src="pics/brainlife_logo.png" />
  </div>
  - ->
  </td>
  </tr>
  </table>
</section>
-->
</section>

<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>
<img style="height:650px;margin-bottom:30px" data-src="pics/ReproSpectrum.png">


----

# Ultimate Goal/Approach

Reproducibility should become merely a *feature* (if not a
side-effect) of the results.
</textarea></section>

<section>
<section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>


# But HOW?

----

## HOWTO: Guiding principles

- Be greedy
  - get as much as possible (even if you think you don't need it)
- Be lazy
  - manually do as little as necessary

----

## HOWTO: Guiding principles

- Be ~~greedy~~ thorough
  - get as much as possible (even if you think you don't need it)
  - know what you are going to do and what you have done
- Be ~~lazy~~ efficient
  - manually do as little as necessary
  - achieve more than originally planned

----

## One more: Pareto Principle


![Pareto principle](pics/webshot-pareto-search.jpg)

more: https://en.wikipedia.org/wiki/Pareto_principle

</textarea>
</section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# What about a Recipe?

----
## Recipe of a Study

- **Ingredients**:
  - humans  (100% of *effort*)
  - computers (0% of *effort*)
  - language(s) (uncounted  *effort*, 100% of the *result*)
    - English/... (human -to- human)
    - programming/scripting languages (human -to- computer)
    - standards (human/computer -to- computer/human)
- **Steps**:
  - **humans**: plan the study ahead
  - **humans and computers**: do data collection/processing

note:
- automate: use language(s) to make computers (not humans) do boring stuff
- human-to-human languages aren't good for automation

----

### Currently Dominant Recipe Effort Proportions

- **Steps**:
  - **humans**: plan the study ahead (<20% effort)
  - **(a good number of) humans and (some) computers**:<br/> do data collection/processing (>80% effort)

----

### Target Recipe Effort Proportions

**Prove Pareto to be *wrong*** <!-- .element: style="color:#ff0000" -->
<br/>**and that we can avoid wasting our effort** <!-- .element: style="color:#ff0000" -->

- **Steps**:
  - **humans**: plan the study ahead (>80% effort)
  - **(some) humans and (many) computers**: <br>
    automated data collection/processing (<20% effort)

----

### Recipe Ingredients for Planing Ahead (>80% effort)

![5 steps](pics/repronim-5steps.png)

----
### Humans: Plan Ahead (>80% effort)

- Plan to be ~~greedy~~ thorough
  - plan for **all** [5 ReproNim steps](http://5steps.repronim.org) (including analyses)
    - prepare to be (ab)used ([Halchenko&Hanke, 2015](http://dx.doi.org/10.1186/s13742-015-0072-7))
    - be ~~lazy~~ efficient and (re)use work of others
    - choose what to (ab)use:
      - language(s), technologies, and approaches
  - choose an RDM (Research Data Management) platform/approach
      - decide how to *log* what you will have done
  - aim to collect rich(er) datasets
- Pre-register
  - treat it as a checklist (now) and a "regression-test" (later) for your plan
- Prepare/train humans to "talk" to computers
  - [ReproNim Training](https://www.repronim.org/teach.html)
  - Listen to and/or participate in
    [BrainHacks](https://brainhack.org/tutorials.html)
  - [DataLad Handbook](http://handbook.datalad.org)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead: Choose an RDM

----

![RDM](pics/google-rdm.png)

----
### Plan Ahead: YODA

![YODA principles](pics/yoda-principles.png)

https://github.com/myyoda/poster/ <br/>by Michael Hanke (CP7, DataLad) et al.

----
### Plan Ahead: YODA's Layout

![YODA layout](pics/yoda-layout.png)

----

### Plan Ahead: YODA's Hierarchy

![YODA Hiearchy](pics/yoda-hierarchy-with-containers.png)

https://github.com/ReproNim/containers/

----

### Example: YODA's DataLad Reproducible Paper

![DataLad repropaper](pics/datalad-handbook-repropaper.png)

http://handbook.datalad.org/en/latest/usecases/reproducible-paper.html <br/>by
ReproNim YODA master Adina Wagner, Michael Hanke, et al.

----

### Fact: ~~No~~Everybody should care about YODA

![fMRIPrep YODA](pics/fmriprep-yoda-PR.png)

https://fmriprep.org by Russ Poldrack (CP5*, OpenNeuro), et al.

----

### Plan Ahead: More on YODA via DataLad

![DataLad Handbook YODA](pics/datalad-handbook-yoda.png)

http://handbook.datalad.org/en/latest/basics/basics-yoda.html

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

# Plan Ahead:<br/> aim to collect rich(er) datasets

----

## Plan Ahead: establish and automate collection<br/> of any relevant (meta)data

- prepare to be (ab)used
  - ~~others~~ you could find (meta)data relevant to their study missing
- more of explained variance = higher power
  - new explanations of "noise" regularly emerge
- manual data entry/wrangling = hard to trace/fix data bugs
- we must be efficient:
  - (ab)use existing solutions
  - seek for low %effort solutions,
- have data ≠ have to analyze all data

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Collect DICOMs (not NIfTI, PAR/REC, ...)

- DICOMs contain lots of relevant metadata
  - most of the contained metadata is not relevant to your study
- Conversion from DICOMs to BIDS could be automated
- You should be able to
  - extract additional metadata happen you need it
  - reconvert if the conversion tool was buggy

----

### Plan Ahead: HeuDiConv/ReproIn

- **HeuDiConv** (https://github.com/nipy/heudiconv)

  - Uses [dcm2niix](https://github.com/rordenlab/dcm2niix/) by
    ReproNim Guru Chris Rorden for basic DICOM -> NIfTIs conversion
  - A flexible scriptable (Python) framework for conversion from
    DICOMs into an arbitrary layout
  - BIDS-aware and comes with a collection of conversion heuristics

- **ReproIn** (https://github.com/repronim/reproin)
  - A convention for organizing and naming sequences on the scanner
    console
	- BIDS-like
	- **very low %effort to start using**
  - HeuDiConv heuristic to convert from such convention to BIDS

----
### Plan Ahead: use ReproIn Convention

![DBIC conversions](pics/dbic-conversions.png)

----
### Plan Ahead: ReproIn or not but automate conversion to BIDS!

- https://github.com/repronim/reproin/
- "Taking Control of your DICOM Data: ReproIn/Heudiconv Tools" ReproIn
  webinar available from https://www.repronim.org/webinar-series.html
- WiP: reproin  helper to streamline handling of multiple studies
- If not ReproIn, consider
  - https://github.com/psychoinformatics-de/datalad-hirni (Hanke, CP7)
  - https://github.com/brainlife/ezbids (Pestilli, Brainlife, CP6*)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Get your hands on Phantom QA Data)
### Surprise: Phantom QA data can explain (some)<br/> variance in your data!<br/>
(Operation code-name: ReproPhantom (?))

----

### FYI: Study "Nuisance"

![f1000](pics/f1000-webshot-20200930.png)

Cheng CP and Halchenko YO. A new virtue of phantom MRI data:
explaining variance in human participant data (v1; under peer
review). F1000Research 2020, 9:1131 <!-- .element: style="font-size:0.6em" -->
https://doi.org/10.12688/f1000research.24544.1

Full slide stack:  <!-- .element: style="font-size:0.6em" -->
http://datasets.datalad.org/centerforopenneuroscience/nuisance/presentations/2020-NNL/

----

### Planned Ahead: (somewhat)

- Phantom Data: DBIC QA ([///dbic/QA](http://datasets.datalad.org/?dir=/dbic/QA))
- Human Data: 206 participants from studies of 3 PIs at DBIC
- DICOM-to-BIDS: [HeuDiConv](https://github.com/nipy/heudiconv/) with [ReproIn](https://github.com/repronim/reproin/) heuristic
- Base OS: [Debian GNU/Linux](http://debian.org) + [NeuroDebian](http://neuro.debian.net)
- QA: [MRIQC](https://github.com/poldracklab/mriqc) (BIDS-App)
- Morphometrics: ["Simple Workflow"](https://github.com/ReproNim/simple_workflow)
  - [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki): BET, FAST, FIRST
  - code/data/details: [10.12688/f1000research.10783.2](http://dx.doi.org/10.12688/f1000research.10783.2)
- Data wrangling and analyses:
  - [Python](http://python.org/), [pandas](https://pandas.pydata.org/), [statsmodels](https://www.statsmodels.org/stable/index.html), [Jupyter notebooks](https://jupyter.org/)
- Containerization: [Singularity](https://singularity.lbl.gov)
- Version control/distribution:
  - [DataLad](http://datalad.org), [datalad-container](http://handbook.datalad.org/en/latest/basics/101-133-containersrun.html?), [///ReproNim/containers](https://github.com/ReproNim/containers)
- Organization: follows [YODA principles](https://github.com/myyoda/poster/blob/master/ohbm2018.pdf)
  - https://github.com/proj-nuisance/nuisance

----
### Model: Phantom SNR "explained"

![Fig 2](pics/f1000-webshot-20200930-fig2.png)

----

### Model: Phantom SNR (variables)

![Table 2](pics/f1000-webshot-20200930-tab2.png)

----

### Model: Gray brain matter

![Fig 3](pics/f1000-webshot-20200930-fig3.png)

----

### Model: Gray brain matter (variables)

![Table 3](pics/f1000-webshot-20200930-tab3.png)

----
### Plan Ahead: Talk to your MR physicist/technician

- they better be doing QA
- do not discard phantom data - can come handy
- **ultra low %effort** to keep phantom data around
- **small %effort** to make use of it

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Physiological data

### Let's streamline acquisition of physiological data<br/>
(Operation code-name: ReproPhys)

----
### Others Planned it already: [phys2bids](https://github.com/physiopy/phys2bids/)

![phys2bids](pics/phys2bids-rtd.png)

A nice overview: [OHBM 2020 poster](https://cdn-akamai.6connex.com/645/1827//phys2bids_OHBM_15922384856589877.pdf)

----
### Others Planned it already: [bidsphysio](https://github.com/cbinyu/bidsphysio)

![bidsphysio](pics/bidsphysio-github.png)

HeuDiConv support PR: https://github.com/nipy/heudiconv/pull/446

----
### Plan Ahead: Consider collecting physiological data

- benefits are known
  - again contribute to the power of your studies
- tools for conversion/slicing are there!
- relatively high **%effort** at the moment to setup/do
  - pales in comparison to other **%effort**s spent

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: "Raw" audio/video stimuli

### Let's collect all video and audio stimuli as presented to the participants<br/>
(Operation code-name: ReproStim)

----
### But WHY/What For?

- QA (was there a jitter/dropped stimuli/randomization...)?
- make it possible to forward model **every** dataset
  - *for resting state - see previous sections*
  - explain low level signal features
  - post-hoc salience features analysis
- **100% reproduce experiment stimulation**

----
### Plowing Ahead: https://github.com/ReproNim/reprostim

- Goal: **0% effort**
  - Minimal **%effort** to setup
  - Fully seamless and automated after that
- HOW:
  - [Video](https://www.amazon.com/gp/product/B00BLZDY6A/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&psc=1)/Audio splitters
  - Video/Audio grabber, e.g. [Magewell USB Capture DVI Plus](https://www.amazon.com/gp/product/B01MSDFAO5/)
    - loseless video codec
  - A new video file upon connect/change of resolution<br>
	 (e.g., `2020.11.24.12.57.08_2020.11.24.15.51.23.mkv`   <!-- .element: style="font-size:0.8em" -->)
  - Synchronization:
    - NTP where possible <br/>(stimuli delivery computer, video grabber, ...)
	- video stream QR time-stamping and detection/decoding
  - Automated "slicing" into BIDS datasets (WiP)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead: Events description

### Let's automate collection of all events information in consistent machine-readable form<br/>
(Operation code-name: ReproEvents)

----
## Still Planning Ahead: Join us

- **There is no generic library/helper yet**
- Target: helper+converters for PsychoPy/PTB-3 to produce rich<br/>
  BIDS [`_events.tsv` + `_events.json`](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/07-behavioral-experiments.html)<br/>
  with [Hierarchical Event Descriptors (HED)](https://github.com/hed-standard/hed-specification)
- But with **little %effort** you should already make you stimuli
  scripts/logs ready<br/> (see e.g., https://github.com/mvdoc/pitcher_localizer)

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

## Plan Ahead Summary

TODO

</textarea></section></section>
<section><section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>

### Reproducible Execution of Data Processing

Becomes more feasible with increased automation <br/>
and good (human AND machine-readable) logging.

----

#### Languages


----

### Recipe extended: "do as little as necessary"


- Plan the study ahead
- (ab)Use work of other humans
  - existing tools
  - software/data distributions
- Automate (make computers do the work for you)
  - acquisition
  - conversion
  - computing
  - logging

----

### Recipe extended: "get as much as possible"

- "meta"data
  - stimuli
  - ...
- auxiliary data
  - phantom QA scans
  - phys recordings


----

### You are (not) special


### Computers: Automated data collection/processing (<20% effort)

TODO:

----

   </textarea>
</section>
</section>



<section>
 <section data-markdown  data-separator="^\n----\n" data-vertical="^\n---\n"><textarea data-template>
## Phantom

### Q1: Why/What?

   - *What factors effect QA metrics?*
   - *How well could we explain variance in QA metrics?*

----

## Phantom: Variables

![Figure 1](pics/f1000-webshot-20200930-fig1.png)

----

## Phantom: Let's model snr_total

- Add a simplistic seasonal model
  - we had only 1 year of data!
  - cos and sin (period: year)
- Simple OLS model
  - variables sequentially orthogonalized in the order presented
  - t-test for individual variables (SAR, Date, etc)
  - F-test for Position (IOPDs) and Seasonal

----

## Phantom
### Model: Total SNR

![Fig 2](pics/f1000-webshot-20200930-fig2.png)

----

## Phantom
### Model: Total SNR (variables)

![Table 2](pics/f1000-webshot-20200930-tab2.png)

----
## Human(s)

- *Q2: How to account for "scanner mood" in human data?*

   - *Can we use phantom QA metrics as a "proxy" of current "effects"?*

----

## Human(s): Let's model total Gray volume

- Basic demographics (age, sex, weight)
- Take Phantom Total SNR, interpolate for human participant scanning dates
  - no other scanner specific factor (e.g., TxRefAmp)
- Add subject position in the scanner
- Add a simplistic seasonal model
- Simple OLS model
  - variables sequentially orthogonalized
  - t-test for individual variables
  - F-test for Position (IOPDs) and Seasonal

----

### Model: Gray brain matter

![Fig 3](pics/f1000-webshot-20200930-fig3.png)

----

### Model: Gray brain matter (variables)

![Table 3](pics/f1000-webshot-20200930-tab3.png)

----

## Human(s)
### Model: CSF, Gray, White + 14 sub-cortical structures

Q3: *What if we check other structures from simple_workflow?*

- Check Total SNR and Seasonal variables only
  - no fishing expedition (yet)
- Result: Nothing passes test after FDR correction
  - only Total SNR for Gray brain matter volume


----

## Conclusions

- *Somewhat surprising*:
  - Variance in SNR through time can **very well** be explained by intrinsic factors
- *Confirming others*:
  - Seasonal variations (but not in 1 year of Phantom)
- *Our hypothesis confirmed*:
  - **A proxy measure of scanner health from phantom data can explain variance in human data results**
- **Include your QA data in your normative databases!**

----

## Future Directions

- Fish a little more (other metrics)
- Validate on new data from DBIC
- Investigate explanatory power of scanning characteristics (TxRefAmp, etc)
- Inspect magnitude of an effect on statistics
- Apply to fMRI data (resting state connectivity)
- Apply to larger (other) datasets (e.g. ABCD, HCP)

</textarea></section></section>



<section>
    <h2>Further Information</h2>
    <ul>
      <li>Data/code:
		<ul>
		  <li><a href="https://github.com/proj-nuisance">github.com/proj-nuisance</a> </li>
		  <li><a href="http://datasets.datalad.org/?dir=/con/nuisance">datasets.datalad.org/?dir=/con/nuisance</a></li>
		  </ul>
      <li>DataLad: <a href="https://datalad.org">datalad.org</a></li>
	  <li>ReproNim: <a href="https://repronim.org">repronim.org</a></li>
    <li>
	  Slides: <ul><li>"Sources": <a href="https://github.com/proj-nuisance/talk-2020-NNL" target="_blank">
			https://github.com/proj-nuisance/talk-2020-NNL</a></li>
		<li>View <a href="http://datasets.datalad.org/con/nuisance/presentations/2020-NNL/#/">here</a></li>
	  </ul>
	</ul>

</section>

<section>
    <h1>Thanks!</h1>
</section>
</section>

</div> <!-- /.slides -->
</div> <!-- /.reveal -->

<script src="reveal.js/js/reveal.js"></script>

<script>
  // Full list of configuration options available at:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width: 1280,
    height: 960,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0,

    controls: true,
    progress: true,
    history: true,
    center: true,

    transition: 'slide', // none/fade/slide/convex/concave/zoom
    // not supported? transition-speed: 'fast',

    // Optional reveal.js plugins
    dependencies: [
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true }
    ],
	 markdown: {
		 smartypants: true
	 }
  });
</script>
</body>
</html>
